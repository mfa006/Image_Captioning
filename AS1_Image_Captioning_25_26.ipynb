{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6wXreGJg2lo"
      },
      "source": [
        "# **Assignment 1 - Image Captioning**\n",
        "\n",
        "<div style=\"border: 3px solid #222; padding: 16px; border-radius: 10px; background-color: #1c1f26; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; color: #e0e0e0;\">\n",
        "  <div style=\"display: flex; align-items: center; gap: 8px; margin-top: 12px;\">\n",
        "    <span style=\"font-size: 24px; color: #ff5555;\">&#128274;</span>\n",
        "    <span style=\"font-size: 16px;\"><strong>Project:</strong> Assignments</span>\n",
        "  </div>\n",
        "  <div style=\"display: flex; align-items: center; gap: 8px; margin-top: 8px;\">\n",
        "    <span style=\"font-size: 20px; color: #ff5555;\">&#128218;</span>\n",
        "    <span style=\"font-size: 16px;\"><strong>Course:</strong> Deep Network Development</span>\n",
        "  </div>\n",
        "  <div style=\"margin-top: 12px; font-size: 14px;\">\n",
        "    <span style=\"font-size: 18px; color: #6e8192;\">&#128100;</span>\n",
        "    <span style=\"font-weight: bold;\"><strong>Authors:</strong></span> Tamás Takács, Imre Molnár (PhD students, Department of Artificial Intelligence, Eötvös Loránd University)\n",
        "  </div>\n",
        "</div>\n",
        "<hr style=\"border: none; border-top: 2px solid #444;\">\n",
        "<br>\n",
        "\n",
        "<img src=\"https://repository-images.githubusercontent.com/83958320/8f162500-8ace-11e9-94ee-0b86d27bbc5e\" alt=\"1\" border=\"0\">\n",
        "\n",
        "This notebook contains the required task for the **first assignment** of the **Deep Network Development (DNDEG)** course. Read the task description carefully and **fill in the empty code cells**.\n",
        "\n",
        "# **Task Description**\n",
        "\n",
        "Your task is to train your **own custom image captioning model** and compare its performance with an existing **pre-trained** model. You will work with the `Flickr8k` [dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k), ensuring it is properly split into **training, validation,** and **test** sets. The model should follow an **Encoder-Decoder + Attention** structure, where the encoder can use a **pre-trained backbone** such as *ResNet*, but it must be **fine-tuned**. An **attention layer** is required to visualize what the network has learned. After training, you will **evaluate and compare** the performance of your model against a **pre-trained** one, analyzing the **generated captions** and the **attention mechanisms**.\n",
        "\n",
        "The `Flickr8k` dataset consists of **8000 images**, each paired with **5 captions**, resulting in a total of **40,000 captions**. It is your **design choice** how you implement the **dataset and batching process**. A common approach is to treat each caption as a separate data point, yielding **40,000 datapoints** in total.\n",
        "\n",
        "The **tokenization process** is also up to you. Since the dataset contains a vast vocabulary, it may include words that appear only once. To improve model efficiency, it is **good practice to limit the vocabulary size**.\n",
        "\n",
        "During **validation and testing**, every image has **5 reference captions**. To ensure a robust `BLEU` **score calculation**, it is recommended to compute `BLEU` scores using **all 5 matching captions** as reference captions for each image.\n",
        "\n",
        "# **Expectations**\n",
        "\n",
        "- Your model is **not expected to achieve state-of-the-art performance**, but it should perform **better than random guessing**. The **loss should decrease** throughout training, and the model’s performance should be monitored to prevent overfitting or underfitting.\n",
        "\n",
        "- You will work with the `Flickr8k` dataset, which should be split into **train, validation, and test** sets. The `training` set is used to train the image captioning model, the `validation` set helps fine-tune hyperparameters and monitor performance, and the `test` set is used for final evaluation.\n",
        "\n",
        "- To assess model performance, include **visualizations of loss values** and **evaluation metrics** for the dataset splits.\n",
        "\n",
        "- You are free to choose any **pre-trained model** (`blip`, `vit-gpt2-flickr8k`, etc.) for model comparison at the end.\n",
        "\n",
        "# **Requirements**\n",
        "\n",
        "- Data Preparation and Visualization:\n",
        "  - Ensure the **Flickr8k dataset** is correctly split into **train, validation, and test sets**.\n",
        "  - Display **sample images with their original captions**.\n",
        "  - Visualize **tokenized captions** to ensure proper preprocessing.\n",
        "\n",
        "- Model Training and Performance Monitoring:\n",
        "  - Train an **Encoder-Decoder + Attention** model using the training set.\n",
        "  - Track **training and validation loss** using visualizations (e.g., `Matplotlib`, `Seaborn`).\n",
        "  - Monitor **BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores** for a greedy decoder throughout training.\n",
        "\n",
        "- Implement **techniques to avoid overfitting**, such as:\n",
        "    - **Early stopping** to halt training when validation loss stops improving.\n",
        "    - **Regularization** (e.g., dropout, weight decay) to improve generalization.\n",
        "    - **Experimenting with different loss functions** to find the most effective approach.\n",
        "    - **Saving the best-performing model** during training for later evaluation.\n",
        "\n",
        "- Model Comparison:\n",
        "  - Evaluate and compare your **custom-trained model** with a **pre-trained model** on the same batch of test images.\n",
        "  - **Visualize generated captions** and compare them to ground-truth captions.\n",
        "  - Compute **BLEU scores** using all five reference captions for a given image.\n",
        "  - Analyze and explain the **strengths and weaknesses** of both models based on performance differences.\n",
        "\n",
        "- Optionally, use **WandB (Weights and Biases)** or **TensorBoard** to:\n",
        "  - Track and visualize **training progress**.\n",
        "  - Monitor **model performance** over time.\n",
        "  - Log **key metrics**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "M7osY_7rZpqa",
        "outputId": "367fd494-eaf7-4583-e9ea-d4079373771c"
      },
      "outputs": [],
      "source": [
        "# # @title Image Captioning Tasks\n",
        "# # %%capture flowchart_output\n",
        "# # HIDDEN CELL\n",
        "# from graphviz import Digraph\n",
        "# from IPython.display import Image as IMG\n",
        "\n",
        "# def create_flowchart(output_filename='flowchart'):\n",
        "#     dot = Digraph(name='Simplified Object Detection', format='png')\n",
        "#     dot.attr(rankdir='LR')\n",
        "#     dot.attr('node', shape='box', style='filled', fontsize='12', fontname='Arial')\n",
        "\n",
        "#     dot.node('Imports', 'Necessary Imports\\n(Easy)', fillcolor='#A0E7A0')\n",
        "\n",
        "#     dot.node('LoadData', 'Load in Your Dataset\\n(Easy)', fillcolor='#A0E7A0')\n",
        "\n",
        "#     dot.node('AugmentData', 'Define Data Augmentations\\n(Easy)', fillcolor='#A0E7A0')\n",
        "\n",
        "#     dot.node('DatasetDataloader', 'Create a Dataset and Dataloaders\\n(Moderate)', fillcolor='#F6D49A')\n",
        "\n",
        "#     dot.node('VisualizeSample', 'Visualize Training Data\\n(Easy)', fillcolor='#A0E7A0')\n",
        "#     dot.node('CreateModel', 'Create the Image Captioning Model\\n(Difficult)', fillcolor='#F6A0A0')\n",
        "\n",
        "#     dot.node('Hyperparameters', 'Define Loss Function and Optimizer\\n(Moderate)', fillcolor='#F6D49A')\n",
        "\n",
        "#     dot.node('TrainModel', 'Train the Image Captioning Model\\n(Difficult)', fillcolor='#F6A0A0')\n",
        "\n",
        "#     dot.node('VisualizeTrain', 'Visualize Training Metrics\\n(Easy)', fillcolor='#A0E7A0')\n",
        "#     dot.node('VisualizeAtt', 'Visualize Attention Weights\\n(Moderate)', fillcolor='#F6D49A')\n",
        "#     dot.node('RunInference', 'Run Inference on the Image Captioning Model\\n(Moderate)', fillcolor='#F6D49A')\n",
        "\n",
        "#     dot.node('LoadModel', 'Load an Existing Image Captioning Model\\n(Easy)', fillcolor='#A0E7A0')\n",
        "\n",
        "#     dot.node('EvaluateModel', 'Evaluate the Existing Image Captioning Model\\n(Moderate)', fillcolor='#F6D49A')\n",
        "\n",
        "#     dot.node('Comparison', 'Compare the Two Models\\n(Difficult)', fillcolor='#F6A0A0')\n",
        "\n",
        "#     # Edges\n",
        "#     dot.edge('Imports', 'LoadData')\n",
        "\n",
        "#     dot.edge('LoadData', 'AugmentData')\n",
        "\n",
        "#     dot.edge('AugmentData', 'DatasetDataloader')\n",
        "\n",
        "#     dot.edge('DatasetDataloader', 'CreateModel')\n",
        "#     dot.edge('DatasetDataloader', 'VisualizeSample')\n",
        "\n",
        "#     dot.edge('CreateModel', 'Hyperparameters')\n",
        "#     dot.edge('Hyperparameters', 'TrainModel')\n",
        "#     dot.edge('TrainModel', 'VisualizeTrain')\n",
        "#     dot.edge('TrainModel', 'VisualizeAtt')\n",
        "#     dot.edge('TrainModel', 'RunInference')\n",
        "\n",
        "#     dot.edge('DatasetDataloader', 'LoadModel')\n",
        "#     dot.edge('LoadModel', 'EvaluateModel')\n",
        "#     dot.edge('RunInference', 'Comparison')\n",
        "#     dot.edge('EvaluateModel', 'Comparison')\n",
        "\n",
        "#     dot.render(output_filename, view=False)\n",
        "\n",
        "# create_flowchart('assignment1_flowchart')\n",
        "# IMG('assignment1_flowchart.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qgNQh32RhZb"
      },
      "source": [
        "Each section of the Notebook will guide you through the task:\n",
        "\n",
        "- `necessary imports`\n",
        "- `data loading process`\n",
        "- `defining data augmentations`\n",
        "- `creating a dataset and dataloaders`\n",
        "- `visualizing the training data`\n",
        "- `creating a CNN model`\n",
        "- `creating a FCN model`\n",
        "- `defining a loss function and an optimizer`\n",
        "- `creating a training and validation loop`\n",
        "- `testing the models`\n",
        "- `comparing results`\n",
        "- `visualizing weights and feature maps`\n",
        "\n",
        "The sections are there to guide you but you **do not have to follow them strictly**.\n",
        "\n",
        "Copy this notebook to your drive (`File -> Save a copy in Drive`), edit it, and upload the final `.ipynb` file to [Canvas](https://canvas.elte.hu). If you are using this in **Google Colab**, save it as `.ipynb` and upload it, or share the public link. If you have your own machine with Jupyter installed, you can work there as well, as long as you save and upload the `.ipynb` file.\n",
        "\n",
        "## **General Rules**\n",
        "Please check all the requirements listed on **Canvas**, but here are some general rules:\n",
        "\n",
        "- The model is **not expected to achieve high performance** but must perform better than random color guessing.\n",
        "- Copying others' code will make you fail the assignment automatically, resulting in a **0**.\n",
        "- Not submitting anything results in a **0**.\n",
        "- Submitting something, as long as it is not an empty notebook, might result in a 1.\n",
        "- **Deadline is November 25th Tuesday 11:59 PM** (strict, no late submission)\n",
        "- Feel free to add more code cells as needed. But don't put code into external Python files.\n",
        "- Please add your `name` and `Neptun ID` below.\n",
        "\n",
        "`Good luck!`\n",
        "\n",
        "**In order to get your grade for an assignment, you must defend it by answering questions during the designated practice time.**\n",
        "\n",
        "## **Guidelines**\n",
        "Please carefully read each cell of the notebook, as they contain guidelines to help you complete the assignments. While you don't have to follow them strictly, we believe that they provide enough help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRoSKcw6SAfk"
      },
      "source": [
        "**Name:Muhammad Faran Akram**  \n",
        "**Neptun ID:dolw0e**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2MZxylcxCF6"
      },
      "source": [
        "## **0. Necessary Imports**\n",
        "Import all the necessary packages for this assignment. **ONLY PYTORCH MODELS ARE ACCEPTED!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in ./.venv/lib/python3.10/site-packages (3.9.2)\n",
            "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: torchaudio in ./.venv/lib/python3.10/site-packages (2.9.1)\n",
            "Requirement already satisfied: torch==2.9.1 in ./.venv/lib/python3.10/site-packages (from torchaudio) (2.9.1)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (2025.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchaudio) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch==2.9.1->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch==2.9.1->torchaudio) (3.0.3)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.9.1)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.10/site-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.10/site-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.10/site-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.10/site-packages (from torch) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (0.24.1)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
            "Requirement already satisfied: torch==2.9.1 in ./.venv/lib/python3.10/site-packages (from torchvision) (2.9.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision) (12.0.0)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (2025.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.10/site-packages (from torch==2.9.1->torchvision) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch==2.9.1->torchvision) (3.0.3)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (3.10.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.10/site-packages (from matplotlib) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.10/site-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install torchaudio\n",
        "!pip install PIL\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/muhammad/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/muhammad/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "import torchvision\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rFTekjbxFAz"
      },
      "source": [
        "## **1. Data Loading Process**\n",
        "\n",
        "For this assignment you will be using the [Flickr8k](https://www.kaggle.com/datasets/adityajn105/flickr8k) dataset, which contains **captions/descriptions** of different images.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/38347541/56469847-9faa0780-645c-11e9-822a-11a15bb56f5b.png\" alt=\"1\" border=\"0\">\n",
        "\n",
        "The best way to download this dataset is through `Kaggle`. First **create a token**, download it and upload it here. Follow [these](https://www.kaggle.com/discussions/general/74235) steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install -q kaggle\n",
        "# !kaggle datasets download -d adityajn105/flickr8k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !unzip *.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqjx0NIwb_4D"
      },
      "source": [
        "## **2. Defining Augmentations**\n",
        "\n",
        "When applying **augmentations** to the `Flickr8k` dataset, it is important to note that these transformations should be applied **only to the images** and not to the captions.\n",
        "\n",
        "Ensure that your **data augmentation pipeline** includes:\n",
        "- A **normalization step** to scale pixel values appropriately.\n",
        "- A **tensor conversion step** to transform images into tensors for model compatibility.\n",
        "- Additional **augmentations of your choice**, such as random cropping, flipping, or color jittering, to enhance model generalization.\n",
        "\n",
        "```python\n",
        "train_transforms = transforms.Compose([\n",
        "            # Add Augmentations\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "            # Add Augmentations\n",
        "])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B8cqnZtblS_i"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.RandomCrop(512),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5FNJamqv_d9"
      },
      "source": [
        "## **3. Creating Datasets and Dataloaders**\n",
        "\n",
        "To load the **Flickr8k** dataset, you need to create a **custom PyTorch** `Dataset` class that returns **images and their corresponding captions**. The captions should be **tokenized** before being returned.\n",
        "\n",
        "Make sure to include special tokens in your tokenized captions:\n",
        "- `sos` (Start of Sentence)\n",
        "- `eos` (End of Sentence)\n",
        "- `unk` (Unknown Token) for words outside the vocabulary.\n",
        "\n",
        "It is recommended to build a **Vocabulary class** to store all the words in your dataset, as your model can only generate words that exist in this vocabulary. However, saving every word is unnecessary. A **common practice** is to include only words that appear **at least 5 times** across the entire dataset to reduce noise and improve model efficiency.\n",
        "\n",
        "For the **DataLoader**, ensure that the **batch size** is appropriate so that it fits into memory. Set the **`shuffle`** parameter as follows:\n",
        "\n",
        "- **Training & Validation DataLoaders:** `shuffle=True` (to randomize the order of samples)  \n",
        "- **Test DataLoader:** `shuffle=False` (to maintain consistency in evaluation)\n",
        "\n",
        "> **Note**: Remember that, a batch has **different examples**, and each example (caption) might have **different sizes**. Therefore, consider padding techniques for the captions.\n",
        "\n",
        "```python\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self,):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raise NotImplementedError\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gClfZ9dZdbBe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
            "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
            "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'a': 4, 'is': 5, 'running': 6, 'dog': 7}\n",
            "[4, 7, 5, 6, 3]\n"
          ]
        }
      ],
      "source": [
        "# ADD YOUR CODE HERE\n",
        "class Vocab():\n",
        "    def __init__(self, catptions,freq_threshold=5):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} # index to string\n",
        "        self.stoi = {v: k for k, v in self.itos.items()} # reverse mapping\n",
        "        self.freqs = {} # word frequencies\n",
        "        self.text=catptions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def tokenizer(self, text):\n",
        "        return nltk.tokenize.word_tokenize(text.lower()) #nltk word tokenizer\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        idx = 4\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                if word not in self.freqs:\n",
        "                    self.freqs[word] = 1\n",
        "                else:\n",
        "                    self.freqs[word] += 1\n",
        "\n",
        "                if self.freqs[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text): #tecxt to indices\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        return [\n",
        "            self.stoi.get(token, self.stoi[\"<UNK>\"])\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "### Test the Vocab class###\n",
        "captions = [\n",
        "    \"A dog is running\",\n",
        "    \"A cat is running\",\n",
        "    \"A dog is sleeping\"\n",
        "]\n",
        "vocab = Vocab(captions, freq_threshold=2)\n",
        "print(vocab.itos)\n",
        "print(vocab.stoi)\n",
        "vocab.build_vocabulary(captions)\n",
        "print(vocab.stoi)\n",
        "print(vocab.numericalize(\"A dog is running fast\")) #fast is <UNK> in captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self,):\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcw52B569Ndf"
      },
      "source": [
        "## **4.1 Visualizing Training Data**\n",
        "\n",
        "To visualize the training data, extract a batch from the training `DataLoader` and plot the **input-target** pairs using `Matplotlib` or `Seaborn`. Ensure that at least **8 pairs** are displayed for a clear representation.\n",
        "\n",
        "Make sure to **visualize the original and the tokenized caption** as well!\n",
        "\n",
        "```python\n",
        "def visualize_batch():\n",
        "  raise NotImplementedError\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRFCR9OFYjUz"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW-8iolJbQz0"
      },
      "source": [
        "## **4.2 Creating the Image Captioning Model**\n",
        "\n",
        "For this assignment, **you are required to create your own custom image captioning model** and **compare its performance** with an existing pre-trained model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Encoder-Attention-Decoder Architecture**\n",
        "\n",
        "Your model should follow an **Encoder-Attention-Decoder** structure with the following components:\n",
        "\n",
        "- The **Encoder** processes images to extract meaningful features. You can either design a **custom convolutional architecture** or use a **pre-trained model** like *ResNet*, but **fine-tuning is required** to adapt the features to the captioning task.\n",
        "- The **Attention Layer** acts as an interface between the encoder and decoder. It leverages the extracted image features to compute **attention scores**, helping the decoder focus on relevant parts of the image during caption generation. For simplicity, use **Linear layers** to implement the attention mechanism.\n",
        "- The **Decoder** is a sequence-based model (e.g., *LSTM*) that processes the image features and generates captions sequentially.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Extra Credit Opportunity**\n",
        "- Additional points will be awarded for implementing either a **Vision Transformer (ViT) encoder** or a **Transformer-based decoder** instead of an LSTM.\n",
        "\n",
        "Here is an example of something similar to what we want to create:\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1wdddaLit7iEyCcVy5bS505NiYzL6c-4x&sz=w1000\">\n",
        "\n",
        "```python\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        return NotImplementedError\n",
        "\n",
        "    def forward(self, images):\n",
        "        return NotImplementedError\n",
        "```\n",
        "\n",
        "```python\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        return NotImplementedError\n",
        "\n",
        "    def forward(self, features, hidden_state):\n",
        "        return NotImplementedError\n",
        "```\n",
        "\n",
        "```python\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, vocab_size, attention_dim, encoder_dim, decoder_dim):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        return NotImplementedError\n",
        "```\n",
        "```python\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim, encoder_dim,decoder_dim):\n",
        "        return NotImplementedError\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        return NotImplementedError\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FApfc3Fab-o-"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOmKet9_iJGp"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLC_HPt0kNHP"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_POIjGsw8us"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXRkD_5VNIxW"
      },
      "source": [
        "## **5. Defining Loss Function and Optimizer**\n",
        "\n",
        "**Loss Functions and their options:**\n",
        "\n",
        "In an **image captioning task**, the model generates a sequence of words conditioned on an image. This involves both **sequence generation (language modeling)** and **image understanding**, making loss selection crucial for effective training. The primary loss function should optimize **word prediction** while ensuring **grammatical correctness and semantic coherence**.\n",
        "\n",
        "Since our task involves **predicting discrete word tokens**, we typically use **sequence-based loss functions**, but we can also explore loss functions that account for **semantic meaning** and **alignment**.\n",
        "\n",
        "---\n",
        "\n",
        "### **a. Cross-Entropy Loss (Standard Sequence Prediction Loss)**  \n",
        "The most common loss for text generation tasks is **Cross-Entropy Loss**, which measures the difference between the **predicted word probability distribution** and the **true word** in the sequence. It is computed as:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{CE} = -\\sum_{t=1}^{T} y_t \\log(\\hat{y}_t)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $T $ is the sequence length (number of words in the caption),\n",
        "- $ y_t $ is the **ground truth word** (one-hot encoded),\n",
        "- $ \\hat{y}_t $ is the **predicted probability** of that word.\n",
        "\n",
        "- **Pros:** Simple, well-established for text generation tasks, easy to implement.\n",
        "- **Cons:** Treats each word prediction independently, ignoring sentence-level meaning, but useful during `teacher forcing`.\n",
        "\n",
        "---\n",
        "\n",
        "### **b. CIDER Loss (Reinforcement Learning-Based Caption Quality Loss)**  \n",
        "Cross-Entropy loss focuses on **token-level** accuracy, but it does not capture **sentence-level fluency and meaning**. CIDER (Consensus-based Image Description Evaluation) helps optimize captions towards human-like descriptions.\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{CIDEr} = 1 - CIDEr(\\hat{Y}, Y)\n",
        "$$\n",
        "\n",
        "where $ CIDEr(\\hat{Y}, Y) $ measures how similar the generated caption $ \\hat{Y} $ is to multiple reference captions $ Y $.\n",
        "\n",
        "- **Pros:** Optimizes for caption similarity to human references, improves fluency. Best suited for **fine-tuning after pretraining with Cross-Entropy** Loss.\n",
        "- **Cons:** Harder to optimize, requires reinforcement learning.\n",
        "\n",
        "---\n",
        "\n",
        "### **c. Reinforcement Learning-Based Loss (REINFORCE with Self-Critical Sequence Training - SCST)**  \n",
        "Since captioning is **sequential**, we can treat it as a reinforcement learning problem. Instead of directly predicting words, we train the model to maximize **rewards** (e.g., BLEU or CIDEr scores). This is done using the **REINFORCE** algorithm:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{RL} = - (r(\\hat{Y}) - r(\\bar{Y})) \\sum_{t=1}^{T} \\log P(y_t | y_{1:t-1}, X)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ r(\\hat{Y}) $ is the reward for the generated caption,\n",
        "- $ r(\\bar{Y}) $ is the baseline reward (e.g., score from a greedy decoder).\n",
        "\n",
        "- **Pros:** Optimizes **sentence-level metrics** instead of per-word accuracy, leading to better captions.  \n",
        "- **Cons:** Computationally expensive, requires careful tuning.   \n",
        "\n",
        "---\n",
        "\n",
        "[PyTorch Documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "\n",
        "---\n",
        "\n",
        "**Optimizers and their options:**\n",
        "\n",
        "There are some pre-built [Optimizers in PyTorch](https://pytorch.org/docs/stable/optim.html), they are sufficient in most cases, especially if their parameters are well set. The two most well-known are Adam (AdamW) and SGD, both of which originate from Gradient Descent, which we implemented earlier.\n",
        "\n",
        "* **S**tochastic **G**radient **D**escent ([SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html))\n",
        "* **ADA**ptive **M**oment optimizer ([ADAM](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html))\n",
        "* [A good general overview](https://www.ruder.io/optimizing-gradient-descent/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl9ubUb-M1Dm"
      },
      "source": [
        "## **6. Training the Image Captioning Model**\n",
        "\n",
        "When implementing the training loop, ensure the following key aspects are included:\n",
        "\n",
        "- Set an **appropriate number of epochs** for model training, balancing between underfitting and overfitting.\n",
        "- Maintain running losses throughout each epoch and compute both **training loss** and **validation loss** per epoch.\n",
        "- Implement an **early stopping mechanism** to halt training if validation loss stops improving, preventing unnecessary overfitting.\n",
        "- **Save the model at its best-performing epoch** based on validation loss, ensuring the best version is retained for inference.\n",
        "\n",
        "> **Note**: Pay attention to your choices. **Be aware of overfitting** and underfitting. Make sure the **loss is decreasing over the epochs**. Save the losses so that they can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ui6WNeRNQoB"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSG_XwNJhfID"
      },
      "source": [
        "## **7.1 Visualizing Training Metrics**\n",
        "\n",
        "- **Restore the model's parameters** from the checkpoint where validation loss was lowest to use the most optimal version of the model.\n",
        "- Use `Matplotlib` or `Seaborn` to plot the loss curves over epochs.\n",
        "\n",
        "Did your model **converge**? Explain your results!\n",
        "\n",
        "```python\n",
        "def plot_losses():\n",
        "  raise NotImplementedError\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3WASFUihhUy"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUSs-XTGhi5b"
      },
      "source": [
        "## **7.2 Visualizing Attention Weights**\n",
        "\n",
        "Visualize the attention weigths in order to understand how your model learned.\n",
        "For that, use the `context` variable returned by the **Attention class** to overlay them on the image.\n",
        "\n",
        "```python\n",
        "def plot_attention():\n",
        "  raise NotImplementedError\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE97iO1e3k1x"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz8-Tr8KXEH8"
      },
      "source": [
        "## **7.3 Running Inference on the Image Captioning Model**\n",
        "\n",
        "Pass test images through the **custom-trained image captioning model**, then evaluate its performance on the test set. Use the **[BLEU score](https://pytorch.org/text/stable/data_metrics.html)** as the evaluation metric, implementing **BLEU-1, BLEU-2, BLEU-3, and BLEU-4**.\n",
        "\n",
        "To improve performance on the validation and test sets, you may use **beam search decoding** instead of standard greedy decoding.  \n",
        "\n",
        "[Beam Search](https://d2l.ai/chapter_recurrent-modern/beam-search.html) is a more sophisticated decoding algorithm that **considers multiple possible caption sequences at each time step instead of selecting the most probable word at every step (greedy decoding)**. It maintains a fixed number of candidate sequences (beam width) and expands them based on their cumulative probabilities, ultimately selecting the **most likely complete caption**.  \n",
        "\n",
        "Using beam search can lead to **more fluent and accurate captions** by avoiding suboptimal word choices that greedy decoding might make.\n",
        "\n",
        "```python\n",
        "def test_model()\n",
        "  raise NotImplementedError\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d71K_sbpYcCB"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJqvIGCKYrz9"
      },
      "source": [
        "## **8. Loading an Existing Image Captioning Model**\n",
        "\n",
        "You are free to select any pre-trained image captioning model available. We recommend considering the following options:\n",
        "\n",
        "1. **BLIP (Bootstrapping Language-Image Pre-training):** BLIP is a versatile vision-language pre-training framework that excels in both understanding and generation tasks. It effectively utilizes noisy web data by generating and filtering synthetic captions, achieving SOTA results in image captioning. Pre-trained models and code are available on [GitHub](https://github.com/salesforce/BLIP) and the [Hugging Face Model Hub](https://huggingface.co/Salesforce/blip-image-captioning-base).\n",
        "\n",
        "2. **ViT-GPT2 Image Captioning Model:** This model combines a Vision Transformer (ViT) as the encoder and GPT-2 as the decoder, effectively connecting visual inputs with text generation. A fine-tuned version on the Flickr8k dataset is accessible on the [Hugging Face Model Hub](https://huggingface.co/NourFakih/image-captioning-Vit-GPT2-Flickr8k)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9QXS8p2Y5Ua"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_UJklMFY7vV"
      },
      "source": [
        "## **9. Evaluating the Existing Image Captioning Model**\n",
        "\n",
        "For evaluation, apply the **same metrics** used in the assessment of your custom image captioning model to ensure a consistent and fair comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9XPhfPMZJvV"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcG6ZkUIZKVb"
      },
      "source": [
        "## **10. Comparing the Two Models**\n",
        "\n",
        "Compare the performance of both models using **BLEU-1, BLEU-2, BLEU-3, and BLEU-4** scores. Visualize predictions from both models on the **same batch of test images** to highlight their differences.\n",
        "\n",
        "Analyze and justify the differences in performance, discussing factors such as model architecture, attention mechanisms, and training strategies. Finally, **propose possible improvements for your custom model**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSKqULM0j8i7"
      },
      "source": [
        "> **Answer**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P7A06srZNnF"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERZsNkx3kNPd"
      },
      "source": [
        "Please make sure to download your `.ipynb` file, and upload it to **Canvas** on time!\n",
        "\n",
        "<img src=\"https://www.usatoday.com/gcdn/authoring/authoring-images/2023/08/25/USAT/70680172007-alertsm.png?crop=2099,1187,x0,y156\" alt=\"1\" border=\"0\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDAqXYIDkPv5",
        "outputId": "caa1707a-bcb8-4924-dfd5-85b7908bd31d"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "deadline = datetime(2025, 11, 25, 23, 59, 0, tzinfo=timezone.utc)\n",
        "\n",
        "def time_until_deadline():\n",
        "    now = datetime.now(timezone.utc)\n",
        "    remaining = deadline - now\n",
        "    if remaining.total_seconds() <= 0:\n",
        "        return \"Time's up!\"\n",
        "    days = remaining.days\n",
        "    hours, remainder = divmod(remaining.seconds, 3600)\n",
        "    minutes, _ = divmod(remainder, 60)\n",
        "\n",
        "    return f\"{days} days, {hours} hours, {minutes} minutes\"\n",
        "\n",
        "print(\"Time left until submission:\", time_until_deadline())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6NlHx5WStAI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
